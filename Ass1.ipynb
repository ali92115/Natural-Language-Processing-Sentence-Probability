{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wasif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "import string\n",
    "from decimal import Decimal\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the things we want to have removed.\n",
    "transl_table = dict( [ (ord(x), ord(y)) for x,y in zip( u\"‘’´“”–-\",  u\"'''\\\"\\\"--\") ] ) \n",
    "\n",
    "filename = 'doyle_Bohemia.txt'\n",
    "file = open(filename, encoding=\"utf8\")\n",
    "text = file.read().replace('\\n', ' ')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up our data file.\n",
    "sentences = sent_tokenize(text)\n",
    "newSentence = [];\n",
    "\n",
    "tempSentence = \"\"\n",
    "for sentence in sentences:\n",
    "    tempSentence = \"\"\n",
    "    temp = sentence.translate(transl_table)\n",
    "    for word in temp.split():\n",
    "        word = word.replace('\"', '')\n",
    "        word = word.replace(',', '')\n",
    "        word = word.replace('.', '')\n",
    "        word = word.replace('\\'', '')\n",
    "        word = word.replace('?', '')\n",
    "        word = word.replace('-', '')\n",
    "        word = word.replace('_', '')\n",
    "        word = word.lower()\n",
    "        tempSentence += word + \" \"\n",
    "    newSentence.append(tempSentence)\n",
    "text_file = open(\"doyle_Bohemia_clean.txt\", \"w\")\n",
    "\n",
    "for sent in newSentence:\n",
    "    n = text_file.write(sent + \"\\n\")\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1 & Part 2\n",
    "\n",
    "#Unique words in our datafile\n",
    "dictionary = {}\n",
    "\n",
    "#total words in Train set\n",
    "total_words = 0\n",
    "\n",
    "#Total words in test set\n",
    "test_words = 0\n",
    "\n",
    "#We have 449 lines, which roughly translates to 359 i.e. 80%\n",
    "train, test = train_test_split(newSentence, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#Find the number of occurances of each word\n",
    "for sentence in train:\n",
    "    for word in sentence.split():\n",
    "        total_words += 1\n",
    "        word = word.lower()\n",
    "        if word not in dictionary:\n",
    "            dictionary[word] = 1\n",
    "        else:\n",
    "            dictionary[word] = dictionary[word] + 1\n",
    "\n",
    "for sentence in test:\n",
    "    for word in sentence.split():\n",
    "        test_words += 1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"unigram_probs.txt\", \"w\")\n",
    "for entry in dictionary:\n",
    "    f.write(entry + \": \" + str(dictionary[entry]/total_words) + \"\\n\")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3\n",
    "bigram_dict = {}\n",
    "\n",
    "#Create counts of bigrams\n",
    "for sentence in train:\n",
    "    word = sentence.split()\n",
    "    for i in range(len(word) - 1):\n",
    "        word[i] = word[i].lower()\n",
    "        if word[i] not in bigram_dict:\n",
    "            bigram_dict[word[i]] = {}\n",
    "            bigram_dict[word[i]][word[i+1]] = 1\n",
    "        else: \n",
    "            if word[i+1] not in bigram_dict[word[i]]:\n",
    "                bigram_dict[word[i]][word[i+1]] = 1\n",
    "            else:\n",
    "                bigram_dict[word[i]][word[i+1]] = bigram_dict[word[i]][word[i+1]] + 1\n",
    "                \n",
    "\n",
    "f = open(\"bigram_probs.txt\", \"w\")\n",
    "for entry in bigram_dict:\n",
    "    for subentry in bigram_dict[entry]:\n",
    "        f.write(entry + \" \" + subentry + \": \" + str(bigram_dict[entry][subentry]/dictionary[entry]) + \"\\n\")\n",
    "    \n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4 Add Smoothing\n",
    "f = open(\"smooth_probs.txt\", \"w\")\n",
    "for entry in bigram_dict:\n",
    "    for subentry in bigram_dict[entry]:\n",
    "        f.write(entry + \" \" + subentry + \": \" + str((bigram_dict[entry][subentry]+0.1)/(dictionary[entry] + 0.1 * len(dictionary))) + \"\\n\")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 5\n",
    "\n",
    "#We will need sentence probabilities under each model for part 6 so we collect those values now.\n",
    "uni_sent = []\n",
    "bi_sent = []\n",
    "smoothbi_sent = []\n",
    "\n",
    "#We calculate the probability of each sentence in the Unigram, Bigram and Smoothened Bigram model.\n",
    "sentprob = 1\n",
    "f = open(\"unigram_eval.txt\", \"w\")\n",
    "for sentence in test:\n",
    "    for word in sentence:\n",
    "        if word.lower() not in dictionary:\n",
    "            sentprob *= 0\n",
    "        else:\n",
    "            sentprob *= Decimal(dictionary[word]/test_words);\n",
    "    f.write(sentence + \" (PROBABILITY): \" + str(sentprob) +\"\\n\")\n",
    "    uni_sent.append(sentprob)\n",
    "    sentprob = 1\n",
    "\n",
    "\n",
    "bigramprob = 1\n",
    "f = open(\"bigram_eval.txt\", \"w\")\n",
    "for sentence in test:\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words) - 1):\n",
    "        if words[i].lower() in bigram_dict:\n",
    "            if words[i+1].lower() in bigram_dict[words[i].lower()]:\n",
    "                bigramprob *= Decimal(bigram_dict[words[i]][words[i+1]]/dictionary[words[i]])\n",
    "            else:\n",
    "                bigramprob *= 0\n",
    "        else:\n",
    "            bigramprob *= 0\n",
    "    f.write(sentence + \" (PROBABILITY): \" + str(bigramprob) +\"\\n\")\n",
    "    bi_sent.append(bigramprob)\n",
    "    bigramprob = 1\n",
    "\n",
    "\n",
    "smoothprob = 1\n",
    "f = open(\"smoothed_eval.txt\", \"w\")\n",
    "for sentence in test:\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words) - 1):\n",
    "        if words[i].lower() in bigram_dict:\n",
    "            if words[i+1].lower() in bigram_dict[words[i].lower()]:\n",
    "                smoothprob *= (bigram_dict[words[i].lower()][words[i+1].lower()]+0.1)/(dictionary[words[i].lower()] + 0.1 * len(dictionary))\n",
    "            else:\n",
    "                smoothprob *= (0+0.1)/(dictionary[entry] + 0.1 * len(dictionary))\n",
    "        else:\n",
    "            smoothprob *= (0.1)/(0.1*len(dictionary))\n",
    "    f.write(sentence + \" (PROBABILITY): \" + str(smoothprob) +\"\\n\")\n",
    "    smoothbi_sent.append(smoothprob)\n",
    "    smoothprob = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we calculate the perplexity of each Model.\n",
    "f = open(\"Readme.txt\", \"w\")\n",
    "f.write(\"Name: Ali Iftakhar \\n \\n\")\n",
    "uni_perp = 1\n",
    "\n",
    "#Calculate P(s_1, s_2, .... s_n)\n",
    "for i in range(len(uni_sent)):\n",
    "    uni_perp *= Decimal(uni_sent[i])\n",
    "        \n",
    "bi_perp = 1\n",
    "#Calculate P(s_1, s_2, .... s_n)\n",
    "for i in range(len(bi_sent)):\n",
    "    bi_perp *= Decimal(bi_sent[i])\n",
    "    \n",
    "smooth_bi_perp = 1.0\n",
    "#Calculate P(s_1, s_2, .... s_n)\n",
    "for i in range(len(smoothbi_sent)):\n",
    "    #This was just an attempt to stomp python's rounding to 0 since the number is really small.\n",
    "    if(smooth_bi_perp * smoothbi_sent[i] != 0):\n",
    "        smooth_bi_perp *= smoothbi_sent[i]\n",
    "\n",
    "\n",
    "if uni_perp == 0:\n",
    "    f.write(\"Perplexity of Unigram: Infinity \\n\")\n",
    "else:\n",
    "    final_uni = (1/uni_perp)**(len(test))\n",
    "    f.write(\"Perplexity of Unigram: \" + final_uni)\n",
    "    \n",
    "\n",
    "if bi_perp == 0:\n",
    "    f.write(\"Perplexity of Bigram: Infinity \\n\")\n",
    "else:\n",
    "    final_bi = (1/bi_perp)**(len(test))\n",
    "    f.write(\"Perplexity of Bigram: \" + final_bi)\n",
    "    \n",
    "#It seems like since smooth_bi_prep is such a miniscule value, it ends up defaulting to infinity, but I am confident that \n",
    "#there exists an answer, but it is just way too big.\n",
    "final_smooth_bi = (1.0/smooth_bi_perp)**(len(test))\n",
    "f.write(\"Perplexity of Smoothened Bigram: \" + str(final_smooth_bi) + \"\\n\")\n",
    "\n",
    "\n",
    "f.write(\"ANSWER TO QUESTION 1 \\n\")\n",
    "f.write(\"I think the worst performance by far, came from the Unigram model. Since there is no dependency of the word to the prior word, we cannot hope to achieve a good prediction. It is unfortunate though that I wasn't aware to get number values for perplexity \\n \\n\")\n",
    "\n",
    "f.write(\"ANSWER TO QUESTION 2 \\n\")\n",
    "f.write(\"I think smoothing definitely helped with the overall performance. For unigram and bigram when we had a 0 probability sentence, it ruined the entire perplexity calculation. Sadly, my probabilities even with smoothing were so low that I wasn't able to get a finite number answer for Smoothened Bigram as well. I'm not sure what I did wrong.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
